{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: Data Extraction\n",
    "\n",
    "Pandas was used to extract the comma sperated value document into a pandas dataform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#reading in the data\n",
    "SCproperties = pd.read_csv('train.csv')\n",
    "unique = pd.read_csv('unique_m.csv')\n",
    "\n",
    "mats = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al',\n",
    "       'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn',\n",
    "       'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb',\n",
    "       'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In',\n",
    "       'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm',\n",
    "       'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta',\n",
    "       'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At',\n",
    "       'Rn']\n",
    "\n",
    "#combining all data into one dataframe\n",
    "all_data= pd.concat([SCproperties, unique], axis=1, join_axes=[SCproperties.index])\n",
    "#print(all_data.shape)\n",
    "all_data = all_data.loc[:,~all_data.columns.duplicated()]\n",
    "#print(all_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: Visialization and Exploration of the Data\n",
    "\n",
    "The CU-O SCs were given a ratio of 2+-0.333 to capture all HTCs with roughly a ratio of 2. \n",
    "\n",
    "The iron SCs were found by simply looking at the SCs that contained iron.\n",
    "\n",
    "The conventional SCs were found by searching for all SCs with a critical temperature of less than 10 K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = unique[(unique > 0)] #This gives a NAN for any value that is zero\n",
    "y = clf.count() #These can then be counted with the count feature\n",
    "#print(y)\n",
    "\n",
    "#Removing the items that are out of place\n",
    "y = y.drop(\"critical_temp\", axis=0)\n",
    "y = y.drop(\"material\", axis=0)\n",
    "\n",
    "#Putting into a dataframe for easiler sorting\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "#Sorting the dataframe from largest to smallest\n",
    "y = y.sort_values(by = [0], ascending = False)\n",
    "\n",
    "#Plotting the twenty most abundant materials, as plotting all of them clogs up the graph. \n",
    "plt.plot(y[0:20])\n",
    "plt.title('Twenty most abundant materials in SCs')\n",
    "plt.show()\n",
    "\n",
    "print('Note: all of the elementents could be plotted, but the x-axis becomes unreadable as the number increases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining the High Temperature Conductors made of Copper and Oxygen\n",
    "#with O-CU ratio roughly 2.0\n",
    "HTC = all_data[(all_data.O/all_data.Cu < 2.333) & (all_data.O/all_data.Cu > 1.667)]\n",
    "#HTC\n",
    "\n",
    "#Finding superconductors with Iron in them\n",
    "Fe = all_data[(all_data.Fe != 0)]\n",
    "#Fe  \n",
    "\n",
    "#Finding conventional superconductors\n",
    "ConvSC = all_data[(all_data.critical_temp < 10)]\n",
    "#ConvSC\n",
    "\n",
    "#Labels for plots\n",
    "item_y = 'mean_Density'\n",
    "item_x = 'critical_temp'\n",
    "\n",
    "types = [HTC,Fe,ConvSC]\n",
    "\n",
    "#print('HTC')\n",
    "x = HTC[item_x]\n",
    "plt.xlabel(item_x)\n",
    "plt.ylabel('Density')\n",
    "hist = x.hist(density = True)\n",
    "plt.title('High Temp SCs with Copper and Oxygen')\n",
    "plt.show()\n",
    "\n",
    "#print('Iron')\n",
    "x = Fe[item_x]\n",
    "plt.xlabel(item_x)\n",
    "plt.ylabel('Density')\n",
    "hist = x.hist(density = True)\n",
    "plt.title('SCs containing Iron')\n",
    "plt.show()\n",
    "\n",
    "#print('Conventional Superconductor')\n",
    "x = ConvSC[item_x]\n",
    "plt.xlabel(item_x)\n",
    "plt.ylabel('Density')\n",
    "hist = x.hist(density = True)\n",
    "plt.title('Conventional SCs with T_crit < 10 K')\n",
    "plt.show()\n",
    "\n",
    "#print('All Superconductors')\n",
    "x = all_data[item_x]\n",
    "plt.xlabel(item_x)\n",
    "plt.ylabel('Density')\n",
    "hist = x.hist(density = True)\n",
    "plt.title('All SCs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: Dimesionality Reduction\n",
    "\n",
    "In the 'unique' dataset, there is one X value and one Y value that are the primary clusters.\n",
    "In the 'properties' dataset, there is a wider spread, but they are mainly clustered at under 15000 in the X component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Project to two dimensions.\n",
    "pcas = PCA(2)\n",
    "#Preserve 50% of cumulative explained variance\n",
    "pca = PCA(0.5)\n",
    "SCproperties_fit = pca.fit(SCproperties)\n",
    "prop_split = pcas.fit_transform(SCproperties)\n",
    "\n",
    "#Plot\n",
    "plt.scatter(prop_split[:, 0], prop_split[:, 1],\n",
    "            c=SCproperties.critical_temp, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('X Component')\n",
    "plt.ylabel('Y Component')\n",
    "plt.title('PCA')\n",
    "plt.colorbar();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique needs to have the \"materials\" column removed to be fitted. \n",
    "unique_pca = unique.drop([\"material\"], axis = 1)\n",
    "\n",
    "#Project to two dimensions.\n",
    "pcas = PCA(2)\n",
    "#Preserve 50% of cumulative explained variance\n",
    "pca = PCA(0.5)\n",
    "unique_fit = pca.fit(unique_pca)\n",
    "unique_split = pcas.fit_transform(unique_pca)\n",
    "\n",
    "#Plot\n",
    "plt.scatter(unique_split[:, 0], unique_split[:, 1],\n",
    "            c=unique.critical_temp, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('X Component')\n",
    "plt.ylabel('Y Component')\n",
    "plt.title('PCA')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Part 4 - Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Setting up the property tables for each category to be analyzed.\n",
    "\n",
    "#All the materials\n",
    "#The inclusion of critical temp and the matieral name threw errors, so they were removed. \n",
    "all_data_props = all_data.loc[:, all_data.columns != 'critical_temp']\n",
    "all_data_props = all_data_props.loc[:, all_data_props.columns != 'material']\n",
    "\n",
    "#High temp copper SCs\n",
    "HTC_props = HTC.loc[:, HTC.columns != 'critical_temp']\n",
    "HTC_props = HTC_props.loc[:, HTC_props.columns != 'material']\n",
    "\n",
    "#Ferric SCs\n",
    "Fe_props = Fe.loc[:, Fe.columns != 'critical_temp']\n",
    "Fe_props = Fe_props.loc[:, Fe_props.columns != 'material']\n",
    "\n",
    "#Conventional SCs\n",
    "ConvSC_props = ConvSC.loc[:, ConvSC.columns != 'critical_temp']\n",
    "ConvSC_props = ConvSC_props.loc[:, ConvSC_props.columns != 'material']\n",
    "\n",
    "temp = all_data[['critical_temp']]\n",
    "temp_HTC = HTC[['critical_temp']]\n",
    "temp_Fe = Fe[['critical_temp']]\n",
    "temp_ConvSC = ConvSC[['critical_temp']]\n",
    "\n",
    "#Setting up the training and testing sets, using all of the data together, and then the individual categories data\n",
    "data_train, data_test, target_train, target_test = train_test_split(all_data_props, temp, test_size = 0.3)\n",
    "\n",
    "data_train_HTC, data_test_HTC, target_train_HTC, target_test_HTC = train_test_split(HTC_props, temp_HTC, test_size = 0.3, random_state = 100)\n",
    "\n",
    "data_train_Fe, data_test_Fe, target_train_Fe, target_test_Fe = train_test_split(Fe_props, temp_Fe, test_size = 0.3, random_state = 100)\n",
    "\n",
    "data_train_ConvSC, data_test_ConvSC, target_train_ConvSC, target_test_ConvSC = train_test_split(ConvSC_props, temp_ConvSC, test_size = 0.3, random_state = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#list(SCproperties)\n",
    "params = list(SCproperties)\n",
    "\n",
    "#Preprocessing - THe floats had to be converted into integers. \n",
    "target_train = target_train.astype('int')\n",
    "target_test= target_test.astype('int')\n",
    "\n",
    "target_train_HTC = target_train_HTC.astype('int')\n",
    "target_test_HTC = target_test_HTC.astype('int')\n",
    "\n",
    "target_train_Fe = target_train_Fe.astype('int')\n",
    "target_test_Fe = target_test_Fe.astype('int')\n",
    "\n",
    "target_train_ConvSC = target_train_ConvSC.astype('int')\n",
    "target_test_ConvSC = target_test_ConvSC.astype('int')\n",
    "\n",
    "\n",
    "#Setting up the random forest classifier.\n",
    "Classifier = RandomForestClassifier(n_estimators = 100, max_depth = 100, random_state = 0)\n",
    "#Classifier = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
    "all_data_class = Classifier.fit(data_train, (target_train))\n",
    "\n",
    "Classifier_HTC = RandomForestClassifier(n_estimators = 100, max_depth = 100, random_state = 0)\n",
    "#Classifier_HTC = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
    "HTC_class = Classifier_HTC.fit(data_train_HTC, (target_train_HTC))\n",
    "\n",
    "Classifier_Fe = RandomForestClassifier(n_estimators = 100, max_depth = 100, random_state = 0)\n",
    "#Classifier_Fe = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
    "Fe_class = Classifier_Fe.fit(data_train_Fe, (target_train_Fe))\n",
    "\n",
    "Classifier_ConvSC = RandomForestClassifier(n_estimators = 100, max_depth = 100, random_state = 0)\n",
    "#Classifier_ConvSC = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
    "ConvSC_class = Classifier_ConvSC.fit(data_train_ConvSC, (target_train_ConvSC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orginally, a max depth of 2 was used for for the random forest classifier, but the predictions were not very accurate. It has been run using a max depth of 2, 5, 10, 50, 100, and then no limit. N_estimators was increased from 100 to 1000. There were few noticable changes once max depth was greater than 50 and n_estimators greater than 100. If the classifier lines above are uncommented, they take significantly more time to run that the current script, with generally the same cross validation scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the critical temps\n",
    "target_pred = all_data_class.predict(data_test)\n",
    "target_pred_HTC = HTC_class.predict(data_test_HTC)\n",
    "target_pred_Fe = Fe_class.predict(data_test_Fe)\n",
    "target_pred_ConvSC = ConvSC_class.predict(data_test_ConvSC)\n",
    "\n",
    "#Setting up lists to be appended, in case multiple runs need to be done and averaged\n",
    "score_all = []\n",
    "score_HTC = []\n",
    "score_Fe = []\n",
    "score_ConvSC = []\n",
    "\n",
    "cv_score_all = []\n",
    "cv_score_HTC = []\n",
    "cv_score_Fe = []\n",
    "cv_score_ConvSC = []\n",
    "\n",
    "#Caluculating the accuracy of the prediction\n",
    "clf = accuracy_score(target_test, target_pred)\n",
    "clf_HTC = accuracy_score(target_test_HTC, target_pred_HTC)\n",
    "clf_Fe = accuracy_score(target_test_Fe, target_pred_Fe)\n",
    "clf_ConvSC = accuracy_score(target_test_ConvSC, target_pred_ConvSC)\n",
    "\n",
    "score_all.append(clf)\n",
    "score_HTC.append(clf_HTC)\n",
    "score_Fe.append(clf_Fe)\n",
    "score_ConvSC.append(clf_ConvSC)\n",
    "\n",
    "#Verifying the RandomTreeClassifier with cross validation. \n",
    "test_cv_score = cross_val_score(all_data_class, data_test, target_test)\n",
    "test_cv_score_HTC = cross_val_score(HTC_class, data_test_HTC, target_test_HTC)\n",
    "test_cv_score_Fe = cross_val_score(Fe_class, data_test_Fe, target_test_Fe)\n",
    "test_cv_score_ConvSC = cross_val_score(ConvSC_class, data_test_ConvSC, target_test_ConvSC)\n",
    "\n",
    "cv_score_all.append(test_cv_score)\n",
    "cv_score_HTC.append(test_cv_score_HTC)\n",
    "cv_score_Fe.append(test_cv_score_Fe)\n",
    "cv_score_ConvSC.append(test_cv_score_ConvSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Displaying importance\n",
    "feature_imp = pd.Series(all_data_class.feature_importances_, index = list(all_data_props)).sort_values(ascending=False)\n",
    "feature_imp_HTC = pd.Series(HTC_class.feature_importances_, index = list(HTC_props)).sort_values(ascending=False)\n",
    "feature_imp_Fe = pd.Series(Fe_class.feature_importances_, index = list(Fe_props)).sort_values(ascending=False)\n",
    "feature_imp_ConvSC = pd.Series(ConvSC_class.feature_importances_, index = list(ConvSC_props)).sort_values(ascending=False)\n",
    "#feature_imp\n",
    "\n",
    "#Displaying the 20 most important features\n",
    "feature_imp_20 = feature_imp[0:20]\n",
    "feature_imp_20_HTC = feature_imp_HTC[0:20]\n",
    "feature_imp_20_Fe = feature_imp_Fe[0:20]\n",
    "feature_imp_20_ConvSC = feature_imp_ConvSC[0:20]\n",
    "\n",
    "sns.barplot(x=feature_imp_20, y=feature_imp.index[0:20])\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features - All Data\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=feature_imp_20_HTC, y=feature_imp_HTC.index[0:20])\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features - HTC\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=feature_imp_20_Fe, y=feature_imp_Fe.index[0:20])\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features - Iron SCs\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=feature_imp_20_ConvSC, y=feature_imp_ConvSC.index[0:20])\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features - Conventional SCs\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('All Data')\n",
    "print(score_all)\n",
    "print(cv_score_all)\n",
    "print()\n",
    "print('HTC')\n",
    "print(score_HTC)\n",
    "print(cv_score_HTC)\n",
    "print()\n",
    "print('Iron SCs')\n",
    "print(score_Fe)\n",
    "print(cv_score_Fe)\n",
    "print()\n",
    "print('Conventional SCs')\n",
    "print(score_ConvSC)\n",
    "print(cv_score_ConvSC)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important features generally agree with the Table 5 in the Hamidieh paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Random forest data\n",
    "plt.scatter(target_test,target_pred)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for All Data\")\n",
    "ylim = (0,150)\n",
    "xlim = (0,150)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(target_test_HTC,target_pred_HTC)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for HTC\")\n",
    "ylim = (0,150)\n",
    "xlim = (0,150)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(target_test_Fe,target_pred_Fe)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for Iron SCs\")\n",
    "ylim = (0,150)\n",
    "xlim = (0,150)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(target_test_ConvSC,target_pred_ConvSC)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for Conventional SCs\")\n",
    "ylim = (0,150)\n",
    "xlim = (0,150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: The plot for the conventional superconductors looks so strange because the floats had to be converted into whole number integers to be handled by the random forest classifier. The spread is not unreasonable for the number of elements contained in this data set, but no density can be determined for each point (there is no depth to each point indicating how many points actually lay at each coordinate).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction of Table 6: I do not know how to insert/find the information needed for those materials. The code I would use to analyse it would be:\n",
    "\n",
    "\"\n",
    "CsBe... = (properties of CsBe)\n",
    "\n",
    "target_pred_CsBe = all_data_class.predict(CsBe)\n",
    "\"\n",
    "\n",
    "and the resulting value would be the predicted temperature. I simply do not know how to find the information, or force the material information into python. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5 - Other machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Running through all categories with the Ridge package to compare to the random forest. \n",
    "reg = Ridge(alpha = 1000).fit(data_train, target_train)\n",
    "reg_test_cr_scorei = cross_val_score(reg, data_test, target_test)\n",
    "reg_test_cr_score = np.mean(reg_test_cr_scorei)\n",
    "\n",
    "target_pred_lin = reg.predict(data_test)\n",
    "\n",
    "reg = Ridge(alpha = 1000).fit(data_train_HTC, target_train_HTC)\n",
    "reg_test_cr_scorei = cross_val_score(reg, data_test_HTC, target_test_HTC)\n",
    "reg_test_cr_score_HTC = np.mean(reg_test_cr_scorei)\n",
    "\n",
    "target_pred_lin_HTC = reg.predict(data_test_HTC)\n",
    "\n",
    "reg = Ridge(alpha = 1000).fit(data_train_Fe, target_train_Fe)\n",
    "reg_test_cr_scorei = cross_val_score(reg, data_test_Fe, target_test_Fe)\n",
    "reg_test_cr_score_Fe = np.mean(reg_test_cr_scorei)\n",
    "\n",
    "target_pred_lin_Fe = reg.predict(data_test_Fe)\n",
    "\n",
    "reg = Ridge(alpha = 100).fit(data_train_ConvSC, target_train_ConvSC)\n",
    "reg_test_cr_scorei = cross_val_score(reg, data_test_ConvSC, target_test_ConvSC)\n",
    "reg_test_cr_score_ConvSC = np.mean(reg_test_cr_scorei)\n",
    "\n",
    "target_pred_lin_ConvSC = reg.predict(data_test_ConvSC)\n",
    "\n",
    "print()\n",
    "print('The average cross_val score comparing the whole data set using Ridge is: ', reg_test_cr_score)\n",
    "print()\n",
    "print('The average cross_val score comparing the whole data set using Random Forests is:', np.mean(cv_score_all))\n",
    "print()\n",
    "print('The average cross_val score comparing the HTC data set using Ridge is:', reg_test_cr_score_HTC)\n",
    "print()\n",
    "print('The average cross_val score comparing the HTC data set using Random Forests is:', np.mean(cv_score_HTC))\n",
    "print()\n",
    "print('The average cross_val score comparing the Iron SC data set using Ridge is:', reg_test_cr_score_Fe)\n",
    "print()\n",
    "print('The average cross_val score comparing the Iron SC data set using Random Forests is:', np.mean(cv_score_Fe))\n",
    "print()\n",
    "print('The average cross_val score comparing the Conventional SC data set using Ridge is:', reg_test_cr_score_ConvSC)\n",
    "print()\n",
    "print('The average cross_val score comparing the Conventional SC set using Random Forests is:', np.mean(cv_score_ConvSC))\n",
    "print()\n",
    "\n",
    "#PLotting\n",
    "plt.ylim(-30,170)\n",
    "plt.xlim(-10,170)\n",
    "plt.scatter(target_test,target_pred_lin)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps with Ridge\")\n",
    "plt.show()\n",
    "\n",
    "plt.ylim(-30,170)\n",
    "plt.xlim(-10,170)\n",
    "plt.scatter(target_test_HTC,target_pred_lin_HTC)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for HTC with Ridge\")\n",
    "plt.show()\n",
    "\n",
    "plt.ylim(-30,170)\n",
    "plt.xlim(-10,170)\n",
    "plt.scatter(target_test_Fe,target_pred_lin_Fe)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for Iron SCs with Ridge\")\n",
    "plt.show()\n",
    "\n",
    "plt.ylim(-1,11)\n",
    "plt.xlim(-1,11)\n",
    "plt.scatter(target_test_ConvSC,target_pred_lin_ConvSC)\n",
    "plt.xlabel('Observed Critical Temp')\n",
    "plt.ylabel('Predicted Critical Temp')\n",
    "plt.title(\"Showing Observed vs Predicted Critical Temps for Conventional SC with Ridge\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge was used as the alternative machine learning technique. It generally yielded better results that the random forest method, at least in terms of the cross-validated score. The difference may be because there were not enough n_estimators, but more estimators than what were used took significant amounts of time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
